{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOING THIS FOR DOCUMENT/CORPUS LEVEL**\n",
    "\n",
    "**IMPORT FROM PDF AND NET. CREATE TEXT FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after installing pdfminer.six\n",
    "#move samples folder from my Data folder\n",
    "#using pdfminer.high_level.extract_text(pdf_file, password='', page_numbers=None,\n",
    "# maxpages=0, caching=True, codec='utf-8', laparams=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = '/Users/lawrence/'\n",
    "print (os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "\n",
    "        print (filename)\n",
    "        text=extract_text(filename)\n",
    "        text=strip_multiple_whitespaces(text)  #may wish to turn this off for readability\n",
    "        ##create text file and open it for writing\n",
    "        #\"w\" letter in our argument, which indicates write and will create a file if it does not exist in library\n",
    "        #Plus sign indicates both read and write.\n",
    "        #The available option beside \"w\" are, \"r\" for read, and \"a\" for append\n",
    "        f= open(filename+\".txt\",\"w+\") \n",
    "        f.write(filename+\"\\n\"+text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a text file from Internet\n",
    "\n",
    "import requests\n",
    "\n",
    "text = requests.get('https://archive.org/stream/ProjectManagementForTheOilAndGasIndustry/ProjectManagementForTheOilAndGasIndustry_djvu.txt').text\n",
    "text=text[70000:300000] #have stripped the front text by inspection only, not by BS\n",
    "text=strip_multiple_whitespaces(text)  #may wish to turn this off for readability\n",
    "filename=\"PM_guidance_for_Energy_Projects\"\n",
    "f= open(filename+\".txt\",\"w+\") \n",
    "f.write(filename+\"\\n\"+text)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USE GENSIM TO CREATE SUMMARIES AND FIND KEY_WORDS** This is adequate for a first pass\n",
    "This may be the place to start when re-running if you have already created PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENSIM summary, keywords\n",
    "Corpus_of_Summaries =[]\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            \n",
    "            content = f.read()\n",
    "            \n",
    "            summary=(filename+'Summary\\n'+summarize(content, ratio=0.01)) #word_count=20\n",
    "            key_words=keywords(content, ratio=0.007)\n",
    "            \n",
    "            print ('\\n\\nFile',filename)\n",
    "            print ('\\nSummary:',summary,\"\\n\")\n",
    "            print ('\\nKeywords:',key_words)\n",
    "            \n",
    "            Corpus_of_Summaries.append(summary)\n",
    "            \n",
    "            #print (content[0:100]) # testing the content is coming through\n",
    "            # print(repr(summary)) #alternate version showing line breaks etc\n",
    "    \n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Corpus_of_Summaries[2]) # Could equally do this w full content, but starting simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDENTIFY TOKENS AND MAKE-UP DICTIONARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words and tokenize.# Here we can add in some odd words we find in the output, or use the NLTK list\n",
    "stoplist = set('for a of the and to in \\uf06e  • \\uf0b7 \\uf0b7 \\uf06e uf09 \\uf09f'.split())\n",
    "Tokens_in_Corpus = [[word for word in summary.lower().split() if word not in stoplist]\n",
    "         for summary in Corpus_of_Summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in Tokens_in_Corpus:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "Frequent_Tokens_in_Corpus= [[token for token in text if frequency[token] > 1] for text in Tokens_in_Corpus]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(Frequent_Tokens_in_Corpus[4:5]) #these slices of lists go up to before the higher number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary, then map from ids to dictionary\n",
    "dictionary = corpora.Dictionary(Frequent_Tokens_in_Corpus)\n",
    "print(dictionary,\"\\n\\n\")\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE BAG OF WORDS MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ie. a list of a list. For each document, we have a list of word frequency for each dictionary item\n",
    "BAG_OF_WORDS_MODEL = [dictionary.doc2bow(text) for text in Frequent_Tokens_in_Corpus]\n",
    "for c in BAG_OF_WORDS_MODEL:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Quick start tutorial. \n",
    "\"Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term referring to a transformation from one document representation to another. In gensim documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. The details of this transformation are learned from the training corpus.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TF-IDF MODEL**\n",
    "One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\n",
    "Let's initialize the tf-idf model, training it on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "TFIDF_MODEL= models.TfidfModel(BAG_OF_WORDS_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TOPIC MODEL via LSI** via CBOW AND TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "#now applying an LSI to the first corpus, by working on top of its representation as a TFIDF\n",
    "# here we have created a two dim LSI space, like Deerwesters 1990 example\n",
    "#Presumably we could create one on top of just the CBOW too\n",
    "lsi_from_TFIDF= models.LsiModel(TFIDF_APPLIED_TO_TRAINING_CORPUS, id2word=dictionary, num_topics=3) # initialize an LSI transformation\n",
    "\n",
    "#It is correct how it has this odd double-barrelled structure: \n",
    "#model = LsiModel(common_corpus, id2word=common_dictionary)\n",
    "# >>> vectorized_corpus = model[common_corpus]  # vectorize input copus in BoW format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the topics\n",
    "lsi_from_TFIDF.print_topics(num_topics=-1, num_words=20) #-1 means show all topics .In significance order. Remember also _ve Contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a single topic as a formatted string with print_topic(topicno, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get as array use lsi.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can Update model with new corpus using add_documents(corpus, chunksize=None, decay=None)\n",
    "#can also save the LSI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TOPIC MODEL LSI - FROM CBOW DIRECTLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_from_CBOW = models.LsiModel(BAG_OF_WORDS_MODEL, id2word=dictionary, num_topics=3) # initialize an LSI transformation\n",
    "TOPIC_MODEL_LSI_APPLIED_TO_CORPUS_FROM_CBOW = lsi_from_CBOW[BAG_OF_WORDS_MODEL] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "lsi_from_CBOW.print_topics(num_topics=-1, num_words=20) #-1 means show all topics .In significance order. Remember also _ve Contribs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING VECTOR REPRESENTATION OF A WHOLE OLD OR NEW CORPUS**\n",
    "To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. In our case, they are the same documents used for training LSI, converted to 3-D LSA space. But that’s only incidental, we might also be indexing a different corpus altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now moved onto Topic and Transformations tutorial\n",
    "#apply tfidf to the trained corpus\n",
    "TFIDF_APPLIED_TO_TRAINING_CORPUS = TFIDF_MODEL[BAG_OF_WORDS_MODEL]\n",
    "for doc in TFIDF_APPLIED_TO_TRAINING_CORPUS:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: LSI FROM TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "TOPIC_MODEL_LSI_from_TFIDF_APPLIED_TO_CORPUS = lsi_from_TFIDF[TFIDF_APPLIED_TO_TRAINING_CORPUS] \n",
    "\n",
    "#particular documents aligned to particular topics\n",
    "for doc in TOPIC_MODEL_LSI_from_TFIDF_APPLIED_TO_CORPUS: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL\n",
    "#this model can now be applied to another corpus other than the training one, not just individaul documents\n",
    "#i have not pulled in a second corpus but this is how you would do it. Note you pull in a corpus (processed as above), not just docs. \n",
    "#corpus2nd_tfidf = TFIDF_MODEL[corpus2nd]\n",
    "# for doc in corpus2nd_tfidf:\n",
    "#   print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: LSI FROM CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_MODEL_LSI_from_CBOW_APPLIED_TO_CORPUS = lsi_from_CBOW[BAG_OF_WORDS_MODEL] \n",
    "\n",
    "for doc in TOPIC_MODEL_LSI_from_CBOW_APPLIED_TO_CORPUS : # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING VECTOR REPRESENTATION OF A SINGLE NEW DOCUMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up above, we had a CBOW representation of each document\n",
    "#We can convert documents to that vector space,once tokenized\n",
    "\n",
    "# eg.This is the announcement of the Sellafield partner programme. https://www.gov.uk/government/news/sellafield-ltd-awards-20-year-project-partnership\n",
    "#Which ONR document is most relevant to this contract ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.gov.uk/government/news/sellafield-ltd-awards-20-year-project-partnership\")\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "new_doc=strip_multiple_whitespaces(soup.get_text())\n",
    "print(new_doc)\n",
    "\n",
    "#page = requests.get(\"https://www.gov.uk/government/news/nda-sets-out-its-grand-challenges\")\n",
    "#page.content[1:300]\n",
    "# need to find which tag works well with this approach. p does not work well with this NEC text \n",
    "#soup.find_all('p')[5].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc=new_doc[0:5487]\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will be considering cosine similarity to determine the similarity of two vectors. Cosine similarity is a standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity measures may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenized documents to vector\n",
    "new_vec_CBOW = dictionary.doc2bow(new_doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_vec_CBOW)  # only those words that match up are given a dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the query to LSI space (based on TFIDF) \n",
    "new_vec_TFIDF=TFIDF_MODEL[new_vec_CBOW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: LSI via TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.39447736857014587), (1, 0.044582505706060725), (2, 0.07041520267680661)]\n"
     ]
    }
   ],
   "source": [
    "new_vec_lsi_fromTFIDF = lsi_from_TFIDF[new_vec_TFIDF]\n",
    "print(new_vec_lsi_fromTFIDF)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **REPRESENTATION OF NEW DOCUMENT: LSI via CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec_lsi_fromCBOW = lsi_from_CBOW[new_vec_CBOW]\n",
    "print(new_vec_lsi_fromCBOW)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COSINE SIMILARITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moved onto Similarity search tutorial.\n",
    "Based on this new doc query,we would like to sort our corpus documents in decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of possible similarities—on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks, just a semantic extension overthe boolean keyword match:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **LSI VIA TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "index = similarities.MatrixSimilarity(lsi_from_TFIDF[TFIDF_APPLIED_TO_TRAINING_CORPUS]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.6954509), (1, 0.6393743), (2, 0.7355601), (3, 0.65083766), (4, 0.6242627), (5, 0.5353125), (6, 0.6523766), (7, 0.5703195), (8, 0.7608185), (9, 0.97505426), (10, 0.5026522), (11, 0.5055611), (12, 0.98621565), (13, 0.6932255), (14, 0.96844214), (15, 0.9984242)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[new_vec_lsi_fromTFIDF] # perform a similarity query against the corpus BASED ON LSI - TDFIDF\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar), so that the first document has a score of 0.99809301 etc.\n",
    "\n",
    "With some standard Python magic we sort these similarities into descending order, and obtain the final answer to the query for Sellafield PPP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 0.9984242), (12, 0.98621565), (9, 0.97505426), (14, 0.96844214), (8, 0.7608185), (2, 0.7355601), (0, 0.6954509), (13, 0.6932255), (6, 0.6523766), (3, 0.65083766), (1, 0.6393743), (4, 0.6242627), (7, 0.5703195), (5, 0.5353125), (11, 0.5055611), (10, 0.5026522)]\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most like\n",
    "print (Corpus_of_Summaries[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least like\n",
    "print (Corpus_of_Summaries[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **LSI VIA CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7372657), (1, 0.7495415), (2, 0.74884546), (3, 0.7513679), (4, 0.7469633), (5, 0.7520852), (6, 0.74919164), (7, 0.7491072), (8, 0.75197256), (9, -0.73850036), (10, 0.7532815), (11, 0.75439507), (12, 0.7511035), (13, 0.7512372), (14, 0.744389), (15, 0.75025356)]\n",
      "\n",
      " [(11, 0.75439507), (10, 0.7532815), (5, 0.7520852), (8, 0.75197256), (3, 0.7513679), (13, 0.7512372), (12, 0.7511035), (15, 0.75025356), (1, 0.7495415), (6, 0.74919164), (7, 0.7491072), (2, 0.74884546), (4, 0.7469633), (14, 0.744389), (0, 0.7372657), (9, -0.73850036)]\n"
     ]
    }
   ],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "index = similarities.MatrixSimilarity(lsi_from_CBOW[TOPIC_MODEL_LSI_APPLIED_TO_CORPUS_FROM_CBOW])\n",
    "sims = index[new_vec_lsi_fromCBOW] \n",
    "print(list(enumerate(sims))) \n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(\"\\n\",sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **TFIDF ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the same but with TFIDF model\n",
    "index = similarities.MatrixSimilarity(TFIDF_APPLIED_TO_TRAINING_CORPUS)\n",
    "new_vec_TFIDF = TFIDF_MODEL[new_vec_CBOW] # convert the query to LSI space (based on TFIDF)\n",
    "print(new_vec_TFIDF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[new_vec_TFIDF] \n",
    "print(list(enumerate(sims)))\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print (\"\\n\")\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the same FROM CBOW DIRECTLY\n",
    "#perform a similarity query against the corpus BASED ON CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the same but with CBOW model\n",
    "index = similarities.MatrixSimilarity(BAG_OF_WORDS_MODEL)\n",
    "sims = index[new_vec_CBOW] \n",
    "print(list(enumerate(sims)))\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print (\"\\n\")\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Using keywords into Neo4j concurrence...**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
